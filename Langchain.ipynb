{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrvOfrfQVpWYXGvTwMnawK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marvelcodeX/Q-A-over-Documents-using-Langchain/blob/main/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install dependencies"
      ],
      "metadata": {
        "id": "Ap1AGydHbqwo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b5P5hHDBUXn1"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community #for different kinds of docs\n",
        "!pip install faiss-cpu #facebook AI similiarity search\n",
        "!pip install pypdf python-docx\n",
        "!pip install sentence-transformers #beacuse dataset is probably txt\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload a document (PDF, DOCX, or TXT)"
      ],
      "metadata": {
        "id": "DkTGBPISbvpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "file_path = list(uploaded.keys())[0]\n",
        "print(f\"Uploaded file path:\", file_path)"
      ],
      "metadata": {
        "id": "1a9MHCQcdCmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the document with LangChain's loaders and split document into smaller chunks"
      ],
      "metadata": {
        "id": "yWvRp2_ob0C6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader, Docx2txtLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "if file_path.endswith(\".pdf\"):\n",
        "  loader = PyPDFLoader(file_path)\n",
        "elif file_path.endswith(\".docx\"):\n",
        "  loader = Docx2txtLoader(file_path)\n",
        "else:\n",
        "  loader = TextLoader(file_path)\n",
        "\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "documents = splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Totla Chunks: {len(documents)}\")"
      ],
      "metadata": {
        "id": "_jRF1ML-emTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create embeddings and store in FAISS vector database"
      ],
      "metadata": {
        "id": "FUrqwV30b3-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectorstore = FAISS.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1JUen6KQloRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load FLAN-T5 model for text generation"
      ],
      "metadata": {
        "id": "QdxqH4hwcCaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "flan_pipeline = pipeline (\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-large\",\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "llm=HuggingFacePipeline(pipeline=flan_pipeline)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ons2D27Pm_OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Retrieval-QA system"
      ],
      "metadata": {
        "id": "jH4BT1y-cGAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\":3}),\n",
        "    chain_type=\"stuff\"\n",
        ")\n",
        "\n",
        "query = \"Give me a short summary of the document\"\n",
        "print(qa.run(query))"
      ],
      "metadata": {
        "id": "WLDAZXF0okxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interactive Q&A loop"
      ],
      "metadata": {
        "id": "fEuMmWoccNfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  q = input(\"Ask a question (or 'exit'): \")\n",
        "  if q.lower() == \"exit\":\n",
        "    break\n",
        "  print(\"Answer: \", qa.run(q))"
      ],
      "metadata": {
        "id": "EHv5yrEBp1we"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}